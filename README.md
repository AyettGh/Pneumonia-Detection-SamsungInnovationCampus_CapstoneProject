# ğŸ©º Lung Infection Detection Model

This repository contains the code for an end-to-end AI system capable of detecting pneumonia from chest X-ray (CXR) images. It uses a YOLOv11 model, orchestrated with Vertex AI and MLflow, and served via a FastAPI backend to a Next.js frontend.

## ğŸ“‚ Project Structure

```text
/lung-infection-detector
â”œâ”€â”€ ğŸ“ backend/                    # FastAPI Application
â”‚   â”œâ”€â”€ ğŸ“ app/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ api.py                 # API router (e.g., /predict)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_client.py       # Client to call Vertex AI Endpoint
â”‚   â”‚   â””â”€â”€ ğŸ“„ schemas.py            # Pydantic request/response models
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile.api           # Dockerfile to deploy backend (e.g., to Cloud Run)
â”‚   â”œâ”€â”€ ğŸ“„ main.py                 # Main FastAPI app entrypoint
â”‚   â””â”€â”€ ğŸ“„ requirements.txt        # Python deps (fastapi, uvicorn, google-cloud-aiplatform)
â”‚
â”œâ”€â”€ ğŸ“ data/                       # Scripts for data handling (not for storing data!)
â”‚   â”œâ”€â”€ ğŸ“„ 01_download_data.py       # Script to fetch NIH dataset
â”‚   â”œâ”€â”€ ğŸ“„ 02_preprocess_dicom.py    # Script to convert DICOM to PNG/JPG
â”‚   â”œâ”€â”€ ğŸ“„ 03_prepare_yolo_labels.py # Script to convert annotations to YOLO format
â”‚   â””â”€â”€ ğŸ“„ 04_upload_to_gcs.py       # Script to upload processed data to GCS for Vertex
â”‚
â”œâ”€â”€ ğŸ“ frontend/                   # Next.js Application
â”‚   â”œâ”€â”€ ğŸ“ components/             # React components (e.g., Upload.tsx, Result.tsx)
â”‚   â”œâ”€â”€ ğŸ“ pages/                  # Next.js pages (index.tsx)
â”‚   â”œâ”€â”€ ğŸ“ services/               # API client (e.g., api.ts to call your FastAPI)
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile.web           # Dockerfile for Next.js app
â”‚   â”œâ”€â”€ ğŸ“„ next.config.js
â”‚   â””â”€â”€ ğŸ“„ package.json
â”‚
â”œâ”€â”€ ğŸ“ model/                      # YOLOv11 Model Training
â”‚   â”œâ”€â”€ ğŸ“ config/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ data.yaml              # YOLO data config (paths to train/val in GCS)
â”‚   â”‚   â””â”€â”€ ğŸ“„ yolov11.yaml            # YOLO model architecture config
â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ train.py                # Main training script (logs with MLflow)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ evaluate.py             # Evaluation script
â”‚   â”‚   â””â”€â”€ ğŸ“„ export.py               # Script to export model to serving format
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile.train          # Dockerfile for Vertex AI Custom Training
â”‚   â””â”€â”€ ğŸ“„ requirements.txt        # Python deps (ultralytics, mlflow, google-cloud-sdk)
â”œâ”€â”€ ğŸ“ notebooks/                  # Jupyter notebooks for exploration & R&D
â”‚   â”œâ”€â”€ ğŸ“„ 01-data-exploration.ipynb
â”‚   â””â”€â”€ ğŸ“„ 02-prototype-model.ipynb
â”œâ”€â”€ ğŸ“ pipelines/                  # Vertex AI Pipelines (KFP)
â”‚   â”œâ”€â”€ ğŸ“ components/             # Reusable pipeline components (e.g., train, deploy)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ train_component.yaml
â”‚   â”‚   â””â”€â”€ ğŸ“„ deploy_component.yaml
â”‚   â”œâ”€â”€ ğŸ“„ pipeline.py               # Main KFP/Vertex AI pipeline definition
â”‚   â””â”€â”€ ğŸ“„ submit_pipeline.py        # Script to compile and submit the pipeline
â”‚
â”œâ”€â”€ ğŸ“„ .dockerignore
â”œâ”€â”€ ğŸ“„ .gitignore                  # IMPORTANT: Ignore data, models, .env, node_modules
â”œâ”€â”€ ğŸ“„ docker-compose.yml          # For local development (runs frontend, backend)
â”œâ”€â”€ ğŸ“„ README.md                   # Project documentation
â””â”€â”€ ğŸ“„ .env.example                # Template for environment variables
```

-----

## âš™ï¸ Workflow and Component Overview

This project is broken down into five core components, each with a specific responsibility.

### 1\. ğŸ“ `data/`

  * **Purpose:** This folder contains scripts for data handling. It **does not** store the actual medical images, which should be in Google Cloud Storage (GCS).
  * **Workflow:**
    1.  **`01_download_data.py`**: Fetches the public NIH dataset.
    2.  **`02_preprocess_dicom.py`**: Converts the DICOM files into a web-friendly format like PNG or JPG.
    3.  **`03_prepare_yolo_labels.py`**: Reads the bounding box annotations and converts them into the `.txt` format that YOLO expects.
    4.  **`04_upload_to_gcs.py`**: Uploads the final `images/` and `labels/` folders to a GCS bucket so Vertex AI can access them for training.

### 2\. ğŸ“ `model/`

  * **Purpose:** This is the heart of the AI. It contains everything needed to train the YOLOv11 model.
  * **Workflow:**
      * **`config/data.yaml`**: This file is configured to point to the `train/` and `val/` paths in your GCS bucket.
      * **`src/train.py`**: This is the main training script, integrated with MLflow.
          * Before training, the MLflow tracking URI is set to Vertex AI Experiments.
          * During training, it logs parameters (e.g., learning rate) using `mlflow.log_params()` and metrics (e.g., mAP, loss) using `mlflow.log_metrics()`.
      * **`Dockerfile.train`**: This crucial file packages the `model/` code and dependencies. Vertex AI Custom Training uses this Dockerfile to create and run a training job in the cloud.

### 3\. ğŸ“ `pipelines/`

  * **Purpose:** This directory orchestrates the entire MLOps workflow using Vertex AI Pipelines.
  * **Workflow:**
      * **`pipeline.py`**: Defines the automated pipeline using the Kubeflow Pipelines (KFP) SDK.
      * This pipeline is a graph (DAG) of components, for example:
        1.  **Data Validation**: Checks if data in GCS is ready.
        2.  **Train Model**: Runs the `Dockerfile.train` (from `/model`) as a custom Vertex AI training job.
        3.  **Evaluate Model**: Checks if the model's mAP (logged via MLflow) is above a set threshold.
        4.  **Deploy Model**: If the evaluation is successful, automatically deploys the best model to a Vertex AI Endpoint.

### 4\. ğŸ“ `backend/`

  * **Purpose:** A FastAPI server that acts as the "middle-man" between the frontend and the cloud-hosted AI model.
  * **Workflow:**
      * This server **does not** run the YOLO model itself, which is essential for scalability.
      * **`model_client.py`** uses the Google Cloud AI Platform SDK to communicate with the deployed model.
      * When a user uploads an image, the `/predict` endpoint in **`api.py`** will:
        1.  Receive the image.
        2.  Send the image to the production **Vertex AI Endpoint** for inference.
        3.  Receive the JSON response (bounding boxes, confidence scores).
        4.  Return this clean JSON to the frontend.
      * This service is containerized with **`Dockerfile.api`** and is designed to be deployed on a serverless platform like Google Cloud Run.

### 5\. ğŸ“ `frontend/`

  * **Purpose:** The user interface, built in Next.js, that allows users to interact with the model.
  * **Workflow:**
    1.  A user visits the webpage (defined in `pages/index.tsx` or `app/page.tsx`).
    2.  They use the **`components/Upload.tsx`** component to upload a chest X-ray.
    3.  The frontend's API client (**`services/api.ts`**) sends this image to the FastAPI backend (`/backend`).
    4.  It receives the bounding box data as JSON from the backend.
    5.  The **`components/Result.tsx`** component displays the original image and draws the bounding boxes on top of it, likely using an HTML `<canvas>` element.

### 6\. ğŸ“ `notebooks/`

  * **Purpose:** This is the "lab" or "scratchpad" for the project. It holds all Jupyter notebooks (.ipynb) used for data exploration, model prototyping, and testing small pieces of code before they are "graduated" into production scripts in the data/ or model/ folders.
  * **Note:** Code in this folder is for R&D and is not part of the production-deployed application.

-----

## ğŸš€ Getting Started

### Local Development

1.  Clone the repository:
    ```bash
    git clone https://github.com/YOUR_USERNAME/lung-infection-detector.git
    cd lung-infection-detector
    ```
2.  Set up environment variables:
    ```bash
    cp .env.example .env
    ```
3.  Fill in the required values in your new `.env` file (GCP project ID, bucket names, etc.).
4.  Build and run the local environment:
    ```bash
    docker-compose up --build
    ```
5.  Access the services:
      * **Frontend**: `http://localhost:3000`
      * **Backend API**: `http://localhost:8000/docs`#   P n e u m o n i a - D e t e c t i o n - S a m s u n g I n n o v a t i o n C a m p u s _ C a p s t o n e P r o j e c t  
 